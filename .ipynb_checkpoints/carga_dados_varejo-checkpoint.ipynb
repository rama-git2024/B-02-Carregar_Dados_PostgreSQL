{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "543e33cb-7792-4a8b-9dc2-83563654059e",
   "metadata": {},
   "source": [
    "## Produção Diário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb7857f8-06a2-444c-a442-bcfab565f201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COPY 2\n",
      "\n",
      "Novos dados carregados com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import docker\n",
    "import tarfile\n",
    "from io import BytesIO\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurações do banco de dados\n",
    "db_config = {\n",
    "    'dbname': 'rama',\n",
    "    'user': 'pcecere',\n",
    "    'password': '244049',\n",
    "    'host': 'localhost',\n",
    "    'port': 5434\n",
    "}\n",
    "\n",
    "# Caminho para o arquivo Excel e CSV\n",
    "excel_file_path = r'X:\\Juridico Varejo\\Reportes\\Diários\\Produção diário Varejo Massificado - 2024.xlsx'\n",
    "sheet_name = 'Dados'  # Altere para o nome da aba correta\n",
    "container_id = '223c59c1268f'  # Substitua pelo ID do seu container\n",
    "\n",
    "def read_and_process_excel(excel_file_path, sheet_name):\n",
    "    df = pd.read_excel(excel_file_path, sheet_name=sheet_name)\n",
    "\n",
    "    rename_df = {\n",
    "        'Data reporte': 'data_reporte',\n",
    "        'Data evento': 'data_evento',\n",
    "        'Carteira': 'carteira',\n",
    "        'CPF/CNPJ': 'cpf_cnpj',\n",
    "        'Nome cliente': 'nome_cliente',\n",
    "        'Escritório': 'escritorio',\n",
    "        'Devedor Principal': 'tipo_devedor_princiapl',\n",
    "        'Dossiê Benner': 'dossie',\n",
    "        'Operações': 'operacoes',\n",
    "        'Tipo ação': 'tipo_acao',\n",
    "        'Indicador' : 'indicador',\n",
    "        'Modalidade' : 'modalidade',\n",
    "        'Revisão' : 'revisao',\n",
    "        'Observações' : 'observacoes',\n",
    "        'Advogado ' : 'advogado',\n",
    "        'Valor envolvido' : 'valor_evolvido',\n",
    "        'Qtde bens retomados' : 'qtde_bens_retomados',\n",
    "        'Placa bem  móvel' : 'placa_bem_movel',\n",
    "        'Data cadastro evento e doc sistema Benner' : 'data_cadastro_evento_benner', \n",
    "        'Poder judiciário' : 'poder_judiciario',\n",
    "        'Rede' : 'rede',\n",
    "        'Honorários' : 'honorarios' \n",
    "    }\n",
    "    df.rename(columns=rename_df, inplace=True)\n",
    "\n",
    "    df = df.dropna(subset=['data_reporte'])\n",
    "\n",
    "    date_columns = ['data_reporte', 'data_evento']\n",
    "    for col in date_columns:\n",
    "        df[col] = pd.to_datetime(df[col]).dt.date\n",
    "\n",
    "    df['valor_evolvido'] = pd.to_numeric(df['valor_evolvido'].str.replace('%', '').str.replace('do faturamento', '').str.strip(), errors='coerce')\n",
    "\n",
    "    # Criando a chave única\n",
    "    df['unique_key'] = df['dossie'] + df['indicador'] + df['data_reporte'].astype(str)\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_existing_records(db_config):\n",
    "    engine = create_engine(f\"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['dbname']}\")\n",
    "    query = \"SELECT dossie, indicador, data_reporte FROM varejo.prod_diario\"\n",
    "    existing_records = pd.read_sql(query, engine)\n",
    "    existing_records['unique_key'] = existing_records['dossie'] + existing_records['indicador'] + existing_records['data_reporte'].astype(str)\n",
    "    return existing_records\n",
    "\n",
    "def save_df_to_csv(df, csv_file_path):\n",
    "    df.to_csv(csv_file_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "def copy_csv_to_docker_container(csv_file_path, container_id):\n",
    "    client = docker.from_env()\n",
    "    container = client.containers.get(container_id)\n",
    "\n",
    "    with BytesIO() as tar_stream:\n",
    "        with tarfile.open(fileobj=tar_stream, mode='w') as tar:\n",
    "            tar.add(csv_file_path, arcname='dados.csv')\n",
    "        tar_stream.seek(0)\n",
    "        container.put_archive('/tmp', tar_stream)\n",
    "\n",
    "def execute_command_in_container(container_id, command):\n",
    "    client = docker.from_env()\n",
    "    container = client.containers.get(container_id)\n",
    "    exec_id = container.exec_run(command)\n",
    "    print(exec_id.output.decode())\n",
    "\n",
    "def insert_new_data(df_new_data, db_config, container_id, csv_file_name):\n",
    "    # Salva o novo dataframe como CSV\n",
    "    df_new_data.to_csv(csv_file_name, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    # Copia o CSV para o container Docker\n",
    "    copy_csv_to_docker_container(csv_file_name, container_id)\n",
    "\n",
    "    # Carrega os novos dados\n",
    "    load_csv_command = f\"psql -U {db_config['user']} -d {db_config['dbname']} -c \\\"COPY varejo.prod_diario FROM '/tmp/{csv_file_name}' DELIMITER ',' CSV HEADER\\\"\"\n",
    "    execute_command_in_container(container_id, load_csv_command)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ler e processar o Excel\n",
    "    df = read_and_process_excel(excel_file_path, sheet_name)\n",
    "\n",
    "    # Obter os registros existentes no banco de dados\n",
    "    existing_records = get_existing_records(db_config)\n",
    "\n",
    "    # Criar a coluna unique_key para verificação\n",
    "    df['unique_key'] = df['dossie'] + df['indicador'] + df['data_reporte'].astype(str)\n",
    "\n",
    "    # Fazer a mesma combinação no banco de dados para comparar\n",
    "    existing_records['unique_key'] = existing_records['dossie'] + existing_records['indicador'] + existing_records['data_reporte'].astype(str)\n",
    "\n",
    "    # Comparar os dados novos com os existentes para identificar novos dossiês\n",
    "    df_new_data = df[~df['unique_key'].isin(existing_records['unique_key'])]\n",
    "\n",
    "    if not df_new_data.empty:\n",
    "        # Remover a coluna unique_key antes de carregar os dados\n",
    "        df_new_data.drop(columns=['unique_key'], inplace=True)\n",
    "\n",
    "        # Se houver novos dados, insira-os no banco\n",
    "        insert_new_data(df_new_data, db_config, container_id, 'dados.csv')\n",
    "        os.remove('dados.csv')\n",
    "        print(\"Novos dados carregados com sucesso!\")\n",
    "    else:\n",
    "        print(\"Nenhum novo dado para carregar.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a16e2be-954f-4995-ad98-814bbeb38169",
   "metadata": {},
   "source": [
    "## Levantamento Alvará Julho 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7389869d-4c7a-4f5a-ae5a-ad702a4656e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nomes das colunas após renomeação:\n",
      "Index(['reporte', 'rede', 'carteira', 'segmento', 'gestor', 'escritorio',\n",
      "       'cliente', 'cnpjcpf', 'op19', 'governamental', 'dossie', 'acao',\n",
      "       'data_do_pedido_da_penhora', 'data_do_deferimento_do_pedido',\n",
      "       'data_do_bloqueio', 'valor_bloqueado', 'devedor_estimado',\n",
      "       'impugnacao_apresentada', 'impugnacao_acolhida',\n",
      "       'qual_materia_da_impugnacao', 'deferimento_de_levantamento',\n",
      "       'recurso_interposto', 'expectativa_de_levantamento', 'status',\n",
      "       'valor_levantado', 'observacao'],\n",
      "      dtype='object')\n",
      "Nenhum novo dado para carregar.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import docker\n",
    "import tarfile\n",
    "from io import BytesIO\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurações do banco de dados\n",
    "db_config = {\n",
    "    'dbname': 'rama',\n",
    "    'user': 'pcecere',\n",
    "    'password': '244049',\n",
    "    'host': 'localhost',\n",
    "    'port': 5434\n",
    "}\n",
    "\n",
    "# Caminho para o arquivo Excel e CSV\n",
    "excel_file_path = r'X:\\Juridico Varejo\\Reportes\\Alvarás_Sisbajud positivo\\Cópia de Levantamento Alvará Julho 2024.xlsx'\n",
    "sheet_name = 'Planilha1'  # Altere para o nome da aba correta\n",
    "container_id = '223c59c1268f'  # Substitua pelo ID do seu container\n",
    "\n",
    "# Dicionário para renomear colunas\n",
    "rename_dict = {\n",
    "    'REPORTE ': 'reporte',\n",
    "    'REDE': 'rede',\n",
    "    'CARTEIRAS': 'carteira',\n",
    "    'SEGMENTO ': 'segmento',\n",
    "    'GESTOR': 'gestor',\n",
    "    'ESCRITÓRIO': 'escritorio',\n",
    "    'CLIENTE ': 'cliente',\n",
    "    'CNPJCPF': 'cnpjcpf',\n",
    "    'OP19': 'op19',\n",
    "    ' GOVERNAMENTAL ': 'governamental',\n",
    "    'DOSSIÊ': 'dossie',\n",
    "    'AÇÃO': 'acao',\n",
    "    'DATA DO PEDIDO DA PENHORA': 'data_do_pedido_da_penhora',\n",
    "    'DATA DO DEFERIMENTO DO PEDIDO': 'data_do_deferimento_do_pedido',\n",
    "    'DATA DO BLOQUEIO': 'data_do_bloqueio',\n",
    "    'VALOR BLOQUEADO': 'valor_bloqueado',\n",
    "    'DEVEDOR INTIMADO ': 'devedor_estimado',\n",
    "    'IMPUGNAÇÃO APRESENTADA ': 'impugnacao_apresentada',\n",
    "    'IMPUGNAÇÃO ACOLHIDA ': 'impugnacao_acolhida', \n",
    "    'QUAL MATÉRIA DA IMPUGNAÇÃO?': 'qual_materia_da_impugnacao',\n",
    "    ' DEFERIMENTO DE LEVANTAMENTO': 'deferimento_de_levantamento',\n",
    "    'RECURSO INTERPOSTO? ': 'recurso_interposto' ,\n",
    "    'EXPECTATIVA DE LEVANTAMENTO' : 'expectativa_de_levantamento',\n",
    "    'STATUS ' : 'status',\n",
    "    'VALOR LEVANTADO' : 'valor_levantado',\n",
    "    'OBSERVAÇÃO ' : 'observacao'\n",
    "}\n",
    "\n",
    "def read_and_process_excel(excel_file_path, sheet_name):\n",
    "    df = pd.read_excel(excel_file_path, sheet_name=sheet_name)\n",
    "\n",
    "    # Renomeia as colunas de acordo com o dicionário\n",
    "    df.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "    # Imprime os nomes das colunas após a renomeação para verificação\n",
    "    print(\"Nomes das colunas após renomeação:\")\n",
    "    print(df.columns)\n",
    "\n",
    "    # Verificar se a coluna 'data_do_pedido_da_penhora' e outras estão presentes\n",
    "    required_columns = ['data_do_pedido_da_penhora', 'reporte', 'data_do_deferimento_do_pedido', 'data_do_bloqueio']\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        raise KeyError(f\"As seguintes colunas estão faltando: {', '.join(missing_columns)}\")\n",
    "\n",
    "    # Drop rows where 'data_do_pedido_da_penhora' is NaN\n",
    "    df = df.dropna(subset=['data_do_pedido_da_penhora'])\n",
    "\n",
    "    # Converter colunas de data\n",
    "    for col in required_columns:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce').dt.date\n",
    "\n",
    "    # Converter colunas para strings e criar a chave única\n",
    "    df['dossie'] = df['dossie'].astype(str)\n",
    "    df['cnpjcpf'] = df['cnpjcpf'].astype(str)\n",
    "    df['reporte'] = df['reporte'].astype(str)\n",
    "    df['unique_key'] = df['dossie'] + df['cnpjcpf'] + df['reporte']\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_existing_records(db_config):\n",
    "    engine = create_engine(f\"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['dbname']}\")\n",
    "    query = \"SELECT dossie, cnpjcpf, reporte FROM varejo.levantamento_alvara\"\n",
    "    existing_records = pd.read_sql(query, engine)\n",
    "    existing_records['unique_key'] = existing_records['dossie'] + existing_records['cnpjcpf'] + existing_records['reporte'].astype(str)\n",
    "    return existing_records\n",
    "\n",
    "def save_df_to_csv(df, csv_file_path):\n",
    "    df.to_csv(csv_file_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "def copy_csv_to_docker_container(csv_file_path, container_id):\n",
    "    client = docker.from_env()\n",
    "    container = client.containers.get(container_id)\n",
    "\n",
    "    with BytesIO() as tar_stream:\n",
    "        with tarfile.open(fileobj=tar_stream, mode='w') as tar:\n",
    "            tar.add(csv_file_path, arcname='dados2.csv')\n",
    "        tar_stream.seek(0)\n",
    "        container.put_archive('/tmp', tar_stream)\n",
    "\n",
    "def execute_command_in_container(container_id, command):\n",
    "    client = docker.from_env()\n",
    "    container = client.containers.get(container_id)\n",
    "    exec_id = container.exec_run(command)\n",
    "    print(exec_id.output.decode())\n",
    "\n",
    "def insert_new_data(df_new_data, db_config, container_id, csv_file_name):\n",
    "    # Salva o novo dataframe como CSV\n",
    "    df_new_data.to_csv(csv_file_name, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    # Copia o CSV para o container Docker\n",
    "    copy_csv_to_docker_container(csv_file_name, container_id)\n",
    "\n",
    "    # Carrega os novos dados\n",
    "    load_csv_command = f\"psql -U {db_config['user']} -d {db_config['dbname']} -c \\\"COPY varejo.levantamento_alvara FROM '/tmp/{csv_file_name}' DELIMITER ',' CSV HEADER\\\"\"\n",
    "    execute_command_in_container(container_id, load_csv_command)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ler e processar o Excel\n",
    "    df = read_and_process_excel(excel_file_path, sheet_name)\n",
    "\n",
    "    # Obter os registros existentes no banco de dados\n",
    "    existing_records = get_existing_records(db_config)\n",
    "\n",
    "    # Criar a coluna unique_key para verificação\n",
    "    df['unique_key'] = df['dossie'] + df['cnpjcpf'] + df['reporte'].astype(str)\n",
    "\n",
    "    # Fazer a mesma combinação no banco de dados para comparar\n",
    "    existing_records['unique_key'] = existing_records['dossie'] + existing_records['cnpjcpf'] + existing_records['reporte'].astype(str)\n",
    "\n",
    "    # Comparar os dados novos com os existentes para identificar novos dossiês\n",
    "    df_new_data = df[~df['unique_key'].isin(existing_records['unique_key'])]\n",
    "\n",
    "    if not df_new_data.empty:\n",
    "        # Remover a coluna unique_key antes de carregar os dados\n",
    "        df_new_data.drop(columns=['unique_key'], inplace=True)\n",
    "\n",
    "        # Se houver novos dados, insira-os no banco\n",
    "        insert_new_data(df_new_data, db_config, container_id, 'dados2.csv')\n",
    "        os.remove('dados2.csv')\n",
    "        print(\"Novos dados carregados com sucesso!\")\n",
    "    else:\n",
    "        print(\"Nenhum novo dado para carregar.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c07bf43-63ac-45da-b45a-895096335759",
   "metadata": {},
   "source": [
    "## BBC_GARANTIA_IMOV 12072018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85be5c25-7590-41c4-963a-decea6e9d183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nomes das colunas após renomeação:\n",
      "Index(['op_19', 'dossie_ativa_1', 'dossie_ativa_2', 'dossie_ativa_3',\n",
      "       'dossie_ativa_4', 'dossie_ativa_5', 'NumeroOperacaoOrigem', 'operacao',\n",
      "       'cliente', 'cd_cpfcnpj', 'tipo_pessoa', 'produto', 'escritorio',\n",
      "       'carteira', 'saldo_atraso_ope', 'desc_categoria_garantia',\n",
      "       'desc_tipo_garantia', 'status_consolidacao_af',\n",
      "       'status_processo_execucao', 'Unnamed: 19', 'Unnamed: 20'],\n",
      "      dtype='object')\n",
      "Nenhum novo dado para carregar.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import docker\n",
    "import tarfile\n",
    "from io import BytesIO\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurações do banco de dados\n",
    "db_config = {\n",
    "    'dbname': 'rama',\n",
    "    'user': 'pcecere',\n",
    "    'password': '244049',\n",
    "    'host': 'localhost',\n",
    "    'port': 5434\n",
    "}\n",
    "\n",
    "# Caminho para o arquivo Excel e CSV\n",
    "excel_file_path = r'X:\\Juridico Varejo\\Reportes\\Imóveis\\Cópia de Cópia de BBC_GARANTIA_IMOV 12072018.xlsx'\n",
    "sheet_name = 'Planilha1'  # Altere para o nome da aba correta\n",
    "container_id = '223c59c1268f'  # Substitua pelo ID do seu container\n",
    "\n",
    "# Dicionário para renomear colunas\n",
    "rename_dict = {\n",
    "    'OP19': 'op_19',\n",
    "    'Dossie_Ativa_1': 'dossie_ativa_1',\n",
    "    'Dossie_Ativa_2': 'dossie_ativa_2',\n",
    "    'Dossie_Ativa_3': 'dossie_ativa_3',\n",
    "    'Dossie_Ativa_4': 'dossie_ativa_4',\n",
    "    'Dossie_Ativa_5': 'dossie_ativa_5',\n",
    "    'NumeroOperacaoOrigem ': 'operacao_original',\n",
    "    'NumeroOperacao': 'operacao',\n",
    "    'Cliente': 'cliente',\n",
    "    'cd_cpfcnpj ': 'cpf_cnpj',\n",
    "    'Tipo_Pessoa': 'tipo_pessoa',\n",
    "    'Produto': 'produto',\n",
    "    'Escritorio_Ranking': 'escritorio',\n",
    "    'Tipo_Carteira': 'carteira',\n",
    "    'SALDO_ATRASO_OPER': 'saldo_atraso_ope',\n",
    "    'DESC_CATEGORIA_GARANTIA': 'desc_categoria_garantia',\n",
    "    'DESC_TIPO_GARANTIA': 'desc_tipo_garantia',\n",
    "    'STATUS ATUAL PROCEDIMENTO EXTRAJUDICIAL DE CONSOLIDAÇÃO - ALIENAÇÃO FIDUCIÁRIA': 'status_consolidacao_af',\n",
    "    'STATUS ATUAL PROCESSO DE EXECUÇÃO': 'status_processo_execucao'\n",
    "}\n",
    "\n",
    "def read_and_process_excel(excel_file_path, sheet_name):\n",
    "    df = pd.read_excel(excel_file_path, sheet_name=sheet_name)\n",
    "\n",
    "    # Renomeia as colunas de acordo com o dicionário\n",
    "    df.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "    # Imprime os nomes das colunas após a renomeação para verificação\n",
    "    print(\"Nomes das colunas após renomeação:\")\n",
    "    print(df.columns)\n",
    "              \n",
    "    # Remover colunas indesejadas\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "    # Converter 'saldo_atraso_ope' para numérico, forçando erros a se tornarem NaN\n",
    "    df['saldo_atraso_ope'] = pd.to_numeric(df['saldo_atraso_ope'], errors='coerce')\n",
    "\n",
    "    # Remover ou tratar valores NaN\n",
    "    df = df.dropna(subset=['saldo_atraso_ope'])\n",
    "\n",
    "    # Dropar colunas completamente vazias\n",
    "    df = df.dropna(axis=1, how='all')\n",
    "    \n",
    "    # Converter colunas para strings e criar a chave única\n",
    "    df['op_19'] = df['op_19'].astype(str)\n",
    "    df['dossie_ativa_1'] = df['dossie_ativa_1'].astype(str)\n",
    "    df['saldo_atraso_ope'] = df['saldo_atraso_ope'].astype(str)\n",
    "    df['unique_key'] = df['op_19'] + df['dossie_ativa_1'] + df['saldo_atraso_ope']\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_existing_records(db_config):\n",
    "    engine = create_engine(f\"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['dbname']}\")\n",
    "    query = \"SELECT op_19, dossie_ativa_1, saldo_atraso_ope FROM varejo.bbc_garantia_imov\"\n",
    "    existing_records = pd.read_sql(query, engine)\n",
    "    existing_records['unique_key'] = existing_records['op_19'] + existing_records['dossie_ativa_1'] + existing_records['saldo_atraso_ope'].astype(str)\n",
    "    return existing_records\n",
    "\n",
    "def save_df_to_csv(df, csv_file_path):\n",
    "    df.to_csv(csv_file_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "def copy_csv_to_docker_container(csv_file_path, container_id):\n",
    "    client = docker.from_env()\n",
    "    container = client.containers.get(container_id)\n",
    "\n",
    "    with BytesIO() as tar_stream:\n",
    "        with tarfile.open(fileobj=tar_stream, mode='w') as tar:\n",
    "            tar.add(csv_file_path, arcname='dados3.csv')\n",
    "        tar_stream.seek(0)\n",
    "        container.put_archive('/tmp', tar_stream)\n",
    "\n",
    "def execute_command_in_container(container_id, command):\n",
    "    client = docker.from_env()\n",
    "    container = client.containers.get(container_id)\n",
    "    exec_id = container.exec_run(command)\n",
    "    print(exec_id.output.decode())\n",
    "\n",
    "def insert_new_data(df_new_data, db_config, container_id, csv_file_name):\n",
    "    # Salva o novo dataframe como CSV\n",
    "    df_new_data.to_csv(csv_file_name, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    # Copia o CSV para o container Docker\n",
    "    copy_csv_to_docker_container(csv_file_name, container_id)\n",
    "\n",
    "    # Carrega os novos dados\n",
    "    load_csv_command = f\"psql -U {db_config['user']} -d {db_config['dbname']} -c \\\"COPY varejo.bbc_garantia_imov FROM '/tmp/{csv_file_name}' DELIMITER ',' CSV HEADER\\\"\"\n",
    "    execute_command_in_container(container_id, load_csv_command)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ler e processar o Excel\n",
    "    df = read_and_process_excel(excel_file_path, sheet_name)\n",
    "\n",
    "    # Obter os registros existentes no banco de dados\n",
    "    existing_records = get_existing_records(db_config)\n",
    "\n",
    "    # Criar a coluna unique_key para verificação\n",
    "    df['unique_key'] = df['op_19'] + df['dossie_ativa_1'] + df['saldo_atraso_ope'].astype(str)\n",
    "\n",
    "    # Fazer a mesma combinação no banco de dados para comparar\n",
    "    existing_records['unique_key'] = existing_records['op_19'] + existing_records['dossie_ativa_1'] + existing_records['saldo_atraso_ope'].astype(str)\n",
    "\n",
    "    # Comparar os dados novos com os existentes para identificar novos dossiês\n",
    "    df_new_data = df[~df['unique_key'].isin(existing_records['unique_key'])]\n",
    "\n",
    "    if not df_new_data.empty:\n",
    "        # Remover a coluna unique_key antes de carregar os dados\n",
    "        df_new_data.drop(columns=['unique_key'], inplace=True)\n",
    "\n",
    "        # Se houver novos dados, insira-os no banco\n",
    "        insert_new_data(df_new_data, db_config, container_id, 'dados3.csv')\n",
    "        os.remove('dados3.csv')\n",
    "        print(\"Novos dados carregados com sucesso!\")\n",
    "    else:\n",
    "        print(\"Nenhum novo dado para carregar.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84788331-c993-4cda-bfc6-145497d4aac4",
   "metadata": {},
   "source": [
    "## Controle de Formalizações Acordos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d15e145a-5de5-477b-9f6e-09bf69b51667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nomes das colunas após renomeação:\n",
      "Index(['indice', 'negociador', 'especificidade', 'numero_acordo', 'operacoes',\n",
      "       'cliente', 'cpf_cnpj', 'tipo_pessoa', 'dossie', 'honorarios',\n",
      "       'reserva_honorarios', 'revisional', 'alvara', 'data_confeccao_minuta',\n",
      "       'previsao_pagamento_entrada', 'status', 'data_implantacao',\n",
      "       'data_indexacao_benner', 'comunicar_acordo', 'data_homologação', 'obs'],\n",
      "      dtype='object')\n",
      "COPY 10\n",
      "\n",
      "Novos dados carregados com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import docker\n",
    "import tarfile\n",
    "from io import BytesIO\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurações do banco de dados\n",
    "db_config = {\n",
    "    'dbname': 'rama',\n",
    "    'user': 'pcecere',\n",
    "    'password': '244049',\n",
    "    'host': 'localhost',\n",
    "    'port': 5434\n",
    "}\n",
    "\n",
    "# Caminho para o arquivo Excel e CSV\n",
    "excel_file_path = r'X:\\Juridico Varejo\\Controle De Formalizações\\2024\\Controle formalizações 2024.xlsx'\n",
    "sheet_name = 'Dados'  # Altere para o nome da aba correta\n",
    "container_id = '223c59c1268f'  # Substitua pelo ID do seu container\n",
    "\n",
    "# Dicionário para renomear colunas\n",
    "rename_dict = {\n",
    "    'Índice': 'indice',\n",
    "    'Negociador': 'negociador',\n",
    "    'Especificidade': 'especificidade',\n",
    "    'Acordo nº': 'numero_acordo',\n",
    "    'Operações': 'operacoes',\n",
    "    'Cliente': 'cliente',\n",
    "    'CPF/CNPJ': 'cpf_cnpj',\n",
    "    'Tipo pessoa': 'tipo_pessoa',\n",
    "    'Dossiê': 'dossie',\n",
    "    'Honorário?': 'honorarios',\n",
    "    'Reserva de honorários': 'reserva_honorarios',\n",
    "    'Revisional?': 'revisional',\n",
    "    'Alvará?': 'alvara',\n",
    "    'Data confecção minuta': 'data_confeccao_minuta',\n",
    "    'Previsão pgto entrada': 'previsao_pagamento_entrada',\n",
    "    'Status': 'status',\n",
    "    'Data implantação': 'data_implantacao',\n",
    "    'Data indexeção Benner': 'data_indexacao_benner',\n",
    "    'Comunicar Acordo': 'comunicar_acordo',\n",
    "    'Data homologação' : 'data_homologação',\n",
    "    'Obs' : 'obs'\n",
    "}\n",
    "\n",
    "def read_and_process_excel(excel_file_path, sheet_name):\n",
    "    df = pd.read_excel(excel_file_path, sheet_name=sheet_name)\n",
    "\n",
    "    # Renomeia as colunas de acordo com o dicionário\n",
    "    df.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "    # Imprime os nomes das colunas após a renomeação para verificação\n",
    "    print(\"Nomes das colunas após renomeação:\")\n",
    "    print(df.columns)\n",
    "\n",
    "    date_columns = ['data_confeccao_minuta', 'previsao_pagamento_entrada', 'data_implantacao', 'data_indexacao_benner', 'comunicar_acordo', 'data_homologação']\n",
    "    for col in date_columns:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce').dt.date\n",
    "\n",
    "    # Remover colunas indesejadas\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "    # Remover ou tratar valores NaN\n",
    "    df = df.dropna(subset=['indice'])\n",
    "\n",
    "    # Dropar colunas completamente vazias\n",
    "    df = df.dropna(axis=1, how='all')\n",
    "    \n",
    "    # Converter colunas para strings e criar a chave única\n",
    "    df['indice'] = df['indice'].astype(str)\n",
    "    df['dossie'] = df['dossie'].astype(str)\n",
    "    df['status'] = df['numero_acordo'].astype(str)\n",
    "    df['unique_key'] = df['indice'] + df['dossie'] + df['numero_acordo']\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_existing_records(db_config):\n",
    "    engine = create_engine(f\"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['dbname']}\")\n",
    "    query = \"SELECT indice, dossie, numero_acordo FROM varejo.controle_formalizacoes_acordos\"\n",
    "    existing_records = pd.read_sql(query, engine)\n",
    "    existing_records['unique_key'] = existing_records['indice'] + existing_records['dossie'] + existing_records['numero_acordo'].astype(str)\n",
    "    return existing_records\n",
    "\n",
    "def save_df_to_csv(df, csv_file_path):\n",
    "    df.to_csv(csv_file_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "def copy_csv_to_docker_container(csv_file_path, container_id):\n",
    "    client = docker.from_env()\n",
    "    container = client.containers.get(container_id)\n",
    "\n",
    "    with BytesIO() as tar_stream:\n",
    "        with tarfile.open(fileobj=tar_stream, mode='w') as tar:\n",
    "            tar.add(csv_file_path, arcname='dados4.csv')\n",
    "        tar_stream.seek(0)\n",
    "        container.put_archive('/tmp', tar_stream)\n",
    "\n",
    "def execute_command_in_container(container_id, command):\n",
    "    client = docker.from_env()\n",
    "    container = client.containers.get(container_id)\n",
    "    exec_id = container.exec_run(command)\n",
    "    print(exec_id.output.decode())\n",
    "\n",
    "def insert_new_data(df_new_data, db_config, container_id, csv_file_name):\n",
    "    # Salva o novo dataframe como CSV\n",
    "    df_new_data.to_csv(csv_file_name, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    # Copia o CSV para o container Docker\n",
    "    copy_csv_to_docker_container(csv_file_name, container_id)\n",
    "\n",
    "    # Carrega os novos dados\n",
    "    load_csv_command = f\"psql -U {db_config['user']} -d {db_config['dbname']} -c \\\"COPY varejo.controle_formalizacoes_acordos FROM '/tmp/{csv_file_name}' DELIMITER ',' CSV HEADER\\\"\"\n",
    "    execute_command_in_container(container_id, load_csv_command)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ler e processar o Excel\n",
    "    df = read_and_process_excel(excel_file_path, sheet_name)\n",
    "\n",
    "    # Obter os registros existentes no banco de dados\n",
    "    existing_records = get_existing_records(db_config)\n",
    "\n",
    "    # Criar a coluna unique_key para verificação\n",
    "    df['unique_key'] = df['indice'] + df['dossie'] + df['numero_acordo'].astype(str)\n",
    "\n",
    "    # Fazer a mesma combinação no banco de dados para comparar\n",
    "    existing_records['unique_key'] = existing_records['indice'] + existing_records['dossie'] + existing_records['numero_acordo'].astype(str)\n",
    "\n",
    "    # Comparar os dados novos com os existentes para identificar novos dossiês\n",
    "    df_new_data = df[~df['unique_key'].isin(existing_records['unique_key'])]\n",
    "\n",
    "    if not df_new_data.empty:\n",
    "        # Remover a coluna unique_key antes de carregar os dados\n",
    "        df_new_data.drop(columns=['unique_key'], inplace=True)\n",
    "\n",
    "        # Se houver novos dados, insira-os no banco\n",
    "        insert_new_data(df_new_data, db_config, container_id, 'dados4.csv')\n",
    "        os.remove('dados4.csv')\n",
    "        print(\"Novos dados carregados com sucesso!\")\n",
    "    else:\n",
    "        print(\"Nenhum novo dado para carregar.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798ca18e-86bc-4c56-849f-788aaef6dd36",
   "metadata": {},
   "source": [
    "## Ajuizamentos 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c0d2a65-2ad7-451e-8a87-5bb855d232aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nomes das colunas após renomeação:\n",
      "Index(['Índice', 'credenciado', 'data_ajuizamento', 'cliente', 'cpf_cnpj',\n",
      "       'tipo_pessoa', 'uf', 'operacao', 'dias_atraso', 'dossie', 'tipo_acao',\n",
      "       'carteira', 'segmento', 'garantia', 'pesquisa_patrimonial', 'sistema',\n",
      "       'valor_acao', 'rede'],\n",
      "      dtype='object')\n",
      "COPY 4\n",
      "\n",
      "Novos dados carregados com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import docker\n",
    "import tarfile\n",
    "from io import BytesIO\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurações do banco de dados\n",
    "db_config = {\n",
    "    'dbname': 'rama',\n",
    "    'user': 'pcecere',\n",
    "    'password': '244049',\n",
    "    'host': 'localhost',\n",
    "    'port': 5434\n",
    "}\n",
    "\n",
    "# Caminho para o arquivo Excel e CSV\n",
    "excel_file_path = r'X:\\Juridico Varejo\\Ajuizamento\\Produção\\Produção 2024\\Planilha produção Varejo - Ajuizamentos 2024.xlsx'\n",
    "sheet_name = 'Dados'  # Altere para o nome da aba correta\n",
    "container_id = '223c59c1268f'  # Substitua pelo ID do seu container\n",
    "\n",
    "# Dicionário para renomear colunas\n",
    "rename_dict = {\n",
    "    'Credenciado': 'credenciado',\n",
    "    'Data do Ajuizamento': 'data_ajuizamento',\n",
    "    'Cliente': 'cliente',\n",
    "    'CPF/CNPJ': 'cpf_cnpj',\n",
    "    'Cliente': 'cliente',\n",
    "    'Tipo pessoa': 'tipo_pessoa',\n",
    "    'UF': 'uf',\n",
    "    'Operação': 'operacao',\n",
    "    'Dias de atraso ': 'dias_atraso',\n",
    "    'Dossiê': 'dossie',\n",
    "    'Tipo de Ação': 'tipo_acao',\n",
    "    'Carteira': 'carteira',\n",
    "    'Segmento': 'segmento',\n",
    "    'Garantia?': 'garantia',\n",
    "    'Pesquisa Patrimonial?': 'pesquisa_patrimonial',\n",
    "    'Sistema': 'sistema',\n",
    "    'Valor da Ação': 'valor_acao',\n",
    "    'Rede': 'rede'\n",
    "}\n",
    "\n",
    "def read_and_process_excel(excel_file_path, sheet_name):\n",
    "    df = pd.read_excel(excel_file_path, sheet_name=sheet_name)\n",
    "\n",
    "    # Renomeia as colunas de acordo com o dicionário\n",
    "    df.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "    # Imprime os nomes das colunas após a renomeação para verificação\n",
    "    print(\"Nomes das colunas após renomeação:\")\n",
    "    print(df.columns)\n",
    "\n",
    "    date_columns = ['data_ajuizamento']\n",
    "    for col in date_columns:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce').dt.date\n",
    "\n",
    "    # Remover colunas indesejadas\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "    # Remover ou tratar valores NaN\n",
    "    df = df.dropna(subset=['data_ajuizamento'])\n",
    "\n",
    "    # Dropar colunas completamente vazias\n",
    "    df = df.dropna(axis=1, how='all')\n",
    "    \n",
    "    # Converter colunas para strings e criar a chave única\n",
    "    df['data_ajuizamento'] = df['data_ajuizamento'].astype(str)  # Converter para string\n",
    "    df['dossie'] = df['dossie'].astype(str)\n",
    "    df['cpf_cnpj'] = df['cpf_cnpj'].astype(str)\n",
    "    df['unique_key'] = df['data_ajuizamento'] + df['dossie'] + df['cpf_cnpj']\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_existing_records(db_config):\n",
    "    engine = create_engine(f\"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['dbname']}\")\n",
    "    query = \"SELECT data_ajuizamento, dossie, cpf_cnpj FROM varejo.ajuizamentos_2024\"\n",
    "    existing_records = pd.read_sql(query, engine)\n",
    "    \n",
    "    # Converter a coluna de data para string antes de concatenar\n",
    "    existing_records['data_ajuizamento'] = existing_records['data_ajuizamento'].astype(str)\n",
    "    existing_records['cpf_cnpj'] = existing_records['cpf_cnpj'].astype(str)\n",
    "    existing_records['dossie'] = existing_records['dossie'].astype(str)\n",
    "    \n",
    "    existing_records['unique_key'] = existing_records['data_ajuizamento'] + existing_records['dossie'] + existing_records['cpf_cnpj']\n",
    "    return existing_records\n",
    "\n",
    "def save_df_to_csv(df, csv_file_path):\n",
    "    df.to_csv(csv_file_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "def copy_csv_to_docker_container(csv_file_path, container_id):\n",
    "    client = docker.from_env()\n",
    "    container = client.containers.get(container_id)\n",
    "\n",
    "    with BytesIO() as tar_stream:\n",
    "        with tarfile.open(fileobj=tar_stream, mode='w') as tar:\n",
    "            tar.add(csv_file_path, arcname='dados5.csv')\n",
    "        tar_stream.seek(0)\n",
    "        container.put_archive('/tmp', tar_stream)\n",
    "\n",
    "def execute_command_in_container(container_id, command):\n",
    "    client = docker.from_env()\n",
    "    container = client.containers.get(container_id)\n",
    "    exec_id = container.exec_run(command)\n",
    "    print(exec_id.output.decode())\n",
    "\n",
    "def insert_new_data(df_new_data, db_config, container_id, csv_file_name):\n",
    "    # Salva o novo dataframe como CSV\n",
    "    df_new_data.to_csv(csv_file_name, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    # Copia o CSV para o container Docker\n",
    "    copy_csv_to_docker_container(csv_file_name, container_id)\n",
    "\n",
    "    # Carrega os novos dados\n",
    "    load_csv_command = f\"psql -U {db_config['user']} -d {db_config['dbname']} -c \\\"COPY varejo.ajuizamentos_2024 FROM '/tmp/{csv_file_name}' DELIMITER ',' CSV HEADER\\\"\"\n",
    "    execute_command_in_container(container_id, load_csv_command)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ler e processar o Excel\n",
    "    df = read_and_process_excel(excel_file_path, sheet_name)\n",
    "\n",
    "    # Obter os registros existentes no banco de dados\n",
    "    existing_records = get_existing_records(db_config)\n",
    "\n",
    "    # Criar a coluna unique_key para verificação no DataFrame processado\n",
    "    df['unique_key'] = df['data_ajuizamento'] + df['dossie'] + df['cpf_cnpj'].astype(str)\n",
    "\n",
    "    # Comparar os dados novos com os existentes para identificar novos dossiês\n",
    "    df_new_data = df[~df['unique_key'].isin(existing_records['unique_key'])]\n",
    "\n",
    "    if not df_new_data.empty:\n",
    "        # Remover a coluna unique_key antes de carregar os dados\n",
    "        df_new_data.drop(columns=['unique_key'], inplace=True)\n",
    "\n",
    "        # Se houver novos dados, insira-os no banco\n",
    "        insert_new_data(df_new_data, db_config, container_id, 'dados5.csv')\n",
    "        os.remove('dados5.csv')\n",
    "        print(\"Novos dados carregados com sucesso!\")\n",
    "    else:\n",
    "        print(\"Nenhum novo dado para carregar.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f070ce33-56a5-4bcc-b25c-1d19bec4f785",
   "metadata": {},
   "source": [
    "## Redirecionamentos Unificado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a595031e-acb4-4721-966e-f6975c07b8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nomes das colunas após renomeação:\n",
      "Index(['data_indicacao', 'cliente', 'cpf_cnpj', 'situacao', 'operacao', 'uf',\n",
      "       'carteira', 'data_solicitacao', 'regularizado'],\n",
      "      dtype='object')\n",
      "Nenhum novo dado para carregar.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import docker\n",
    "import tarfile\n",
    "from io import BytesIO\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurações do banco de dados\n",
    "db_config = {\n",
    "    'dbname': 'rama',\n",
    "    'user': 'pcecere',\n",
    "    'password': '244049',\n",
    "    'host': 'localhost',\n",
    "    'port': 5434\n",
    "}\n",
    "\n",
    "# Caminho para o arquivo Excel e CSV\n",
    "excel_file_path = r'X:\\Recuperação\\Indicações\\Indicações 2024\\Redirecionamentos - 2024\\Redirecionamentos Unificado.xlsx'\n",
    "sheet_name = 'Planilha1'  # Altere para o nome da aba correta\n",
    "container_id = '223c59c1268f'  # Substitua pelo ID do seu container\n",
    "\n",
    "# Dicionário para renomear colunas\n",
    "rename_dict = {\n",
    "    'Data de indicação': 'data_indicacao',\n",
    "    'Cliente': 'cliente',\n",
    "    'CPF/CNPJ': 'cpf_cnpj',\n",
    "    'Situação': 'situacao',\n",
    "    'Operação': 'operacao',\n",
    "    'UF': 'uf',\n",
    "    'Carteira': 'carteira',\n",
    "    'Solicitado em': 'data_solicitacao',\n",
    "    'Regularizado?': 'regularizado'\n",
    "}\n",
    "\n",
    "def read_and_process_excel(excel_file_path, sheet_name):\n",
    "    df = pd.read_excel(excel_file_path, sheet_name=sheet_name)\n",
    "\n",
    "    # Renomeia as colunas de acordo com o dicionário\n",
    "    df.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "    # Imprime os nomes das colunas após a renomeação para verificação\n",
    "    print(\"Nomes das colunas após renomeação:\")\n",
    "    print(df.columns)\n",
    "\n",
    "    date_columns = ['data_indicacao', 'data_solicitacao']\n",
    "    for col in date_columns:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce').dt.date\n",
    "\n",
    "    # Remover colunas indesejadas\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "    # Remover ou tratar valores NaN\n",
    "    df = df.dropna(subset=['data_indicacao'])\n",
    "\n",
    "    # Dropar colunas completamente vazias\n",
    "    df = df.dropna(axis=1, how='all')\n",
    "    \n",
    "    # Converter colunas para strings e criar a chave única\n",
    "    df['data_indicacao'] = df['data_indicacao'].astype(str)  # Converter para string\n",
    "    df['operacao'] = df['operacao'].astype(str)\n",
    "    df['cpf_cnpj'] = df['cpf_cnpj'].astype(str)\n",
    "    df['unique_key'] = df['data_indicacao'] + df['operacao'] + df['cpf_cnpj']\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_existing_records(db_config):\n",
    "    engine = create_engine(f\"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['dbname']}\")\n",
    "    query = \"SELECT data_indicacao, operacao, cpf_cnpj FROM varejo.redirecionamentos_unificado\"\n",
    "    existing_records = pd.read_sql(query, engine)\n",
    "\n",
    "    # Convertendo as colunas de data para string antes da concatenação\n",
    "    existing_records['data_indicacao'] = existing_records['data_indicacao'].astype(str)\n",
    "    existing_records['operacao'] = existing_records['operacao'].astype(str)\n",
    "    existing_records['cpf_cnpj'] = existing_records['cpf_cnpj'].astype(str)\n",
    "\n",
    "    existing_records['unique_key'] = existing_records['data_indicacao'] + existing_records['operacao'] + existing_records['cpf_cnpj']\n",
    "    return existing_records\n",
    "\n",
    "def save_df_to_csv(df, csv_file_path):\n",
    "    df.to_csv(csv_file_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "def copy_csv_to_docker_container(csv_file_path, container_id):\n",
    "    client = docker.from_env()\n",
    "    container = client.containers.get(container_id)\n",
    "\n",
    "    with BytesIO() as tar_stream:\n",
    "        with tarfile.open(fileobj=tar_stream, mode='w') as tar:\n",
    "            tar.add(csv_file_path, arcname='dados6.csv')\n",
    "        tar_stream.seek(0)\n",
    "        container.put_archive('/tmp', tar_stream)\n",
    "\n",
    "def execute_command_in_container(container_id, command):\n",
    "    client = docker.from_env()\n",
    "    container = client.containers.get(container_id)\n",
    "    exec_id = container.exec_run(command)\n",
    "    print(exec_id.output.decode())\n",
    "\n",
    "def insert_new_data(df_new_data, db_config, container_id, csv_file_name):\n",
    "    # Remove a coluna 'indice' se estiver presente\n",
    "    if 'indice' in df_new_data.columns:\n",
    "        df_new_data.drop(columns=['indice'], inplace=True)\n",
    "    \n",
    "    # Salva o novo dataframe como CSV\n",
    "    df_new_data.to_csv(csv_file_name, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    # Copia o CSV para o container Docker\n",
    "    copy_csv_to_docker_container(csv_file_name, container_id)\n",
    "\n",
    "    # Carrega os novos dados\n",
    "    load_csv_command = f\"psql -U {db_config['user']} -d {db_config['dbname']} -c \\\"COPY varejo.redirecionamentos_unificado (data_indicacao, cliente, cpf_cnpj, situacao, operacao, uf, carteira, data_solicitacao, regularizado) FROM '/tmp/{csv_file_name}' DELIMITER ',' CSV HEADER\\\"\"\n",
    "    execute_command_in_container(container_id, load_csv_command)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ler e processar o Excel\n",
    "    df = read_and_process_excel(excel_file_path, sheet_name)\n",
    "\n",
    "    # Obter os registros existentes no banco de dados\n",
    "    existing_records = get_existing_records(db_config)\n",
    "\n",
    "    # Criar a coluna unique_key para verificação\n",
    "    df['unique_key'] = df['data_indicacao'] + df['operacao'] + df['cpf_cnpj'].astype(str)\n",
    "\n",
    "    # Fazer a mesma combinação no banco de dados para comparar\n",
    "    existing_records['unique_key'] = existing_records['data_indicacao'] + existing_records['operacao'] + existing_records['cpf_cnpj'].astype(str)\n",
    "\n",
    "    # Comparar os dados novos com os existentes para identificar novos dossiês\n",
    "    df_new_data = df[~df['unique_key'].isin(existing_records['unique_key'])]\n",
    "\n",
    "    if not df_new_data.empty:\n",
    "        # Remover a coluna unique_key antes de carregar os dados\n",
    "        df_new_data.drop(columns=['unique_key'], inplace=True)\n",
    "\n",
    "        # Se houver novos dados, insira-os no banco\n",
    "        insert_new_data(df_new_data, db_config, container_id, 'dados6.csv')\n",
    "        os.remove('dados6.csv')\n",
    "        print(\"Novos dados carregados com sucesso!\")\n",
    "    else:\n",
    "        print(\"Nenhum novo dado para carregar.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281667df-ca34-4a52-b2a3-9365a417e3c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
